# import the necessary packages
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.python.keras.models import load_model
from utils.iou import compute_iou
from utils.ap import compute_ap
from utils import config
import matplotlib.pyplot as plt
from pycocotools.coco import COCO
from imutils import paths
import numpy as np
import random
import cv2
import pickle
import os

# load label binarizer
lb = pickle.loads(open(config.ENCODER_PATH, "rb").read())

model = load_model(config.MODEL_PATH)
print("[INFO] evaluating object detector...")
# make predictions on the testing set and calculate the AP and mAP matrices
print("[INFO] loading testing images...")
# grab all image paths in the test images directory
TestImagePaths = list(paths.list_images(config.TEST_IMAGES))
# using cocoAPI to load coco annotation information
coco = COCO(config.TEST_ANNOTS)
# load COCO categories and super categories
cats = coco.loadCats(coco.getCatIds())
catIds = [cat['id'] for cat in cats]

NUM_CLASS = len(lb.classes_)
# MIN_IOU: minimum threshold of iou to determine whether a roi is TP or NP
MIN_IOU = 0.5
# AP: the area under the precision-recall curve, one class one AP
AP = np.zeros(NUM_CLASS)
# TP_FP: list of TP(1) and NP(0)
TP_FP = [[] for i in range(NUM_CLASS)]
# TP_FN: TP + FN = total num of ground-truth of each class
TP_FN = np.zeros((NUM_CLASS, 1))
# Precision: TP / (TP + FP)
Precision = [[] for i in range(NUM_CLASS)]
# Recall: TP / (TP + FN)
Recall = [[] for i in range(NUM_CLASS)]
# print(lb.classes_)

# loop over testing image paths
for (i, TestImagePath) in enumerate(TestImagePaths):
    # show a progress report
    print("[INFO] evaluating testing image {}/{}...".format(i + 1, len(TestImagePaths)))
    filename = TestImagePath.split(os.path.sep)[-1]
    filename = filename[:filename.rfind(".")]
    img_id = int(filename.lstrip('0'))
    annIds = coco.getAnnIds(imgIds=img_id, iscrowd=None)
    annInfos = coco.loadAnns(annIds)
    # initialize our list of ground-truth bounding boxes
    gtBoxes = []
    # coco bounding box format: (x - top left, y - top left, width, height)
    # loop over all ground-truth 'object' elements in testing images
    for annInfo in annInfos:
        # extract the label and bounding box coordinates
        idx_gt = annInfo['category_id']
        cat = cats[catIds.index(idx_gt)]['name']
        # record num of ground-truth in each class
        label_id_gt = np.where(lb.classes_ == cat)
        print(np.asscalar(label_id_gt[0]))
        TP_FN[label_id_gt] += 1
        xMin, yMin, bw, bh = annInfo['bbox']
        xMax = xMin + bw
        yMax = yMin + bh
        # update our list of ground-truth bounding boxes
        gtBoxes.append((xMin, yMin, xMax, yMax, np.asscalar(label_id_gt[0])))
    # load the input image from disk
    image = cv2.imread(TestImagePath)
    # run selective search on the image and initialize our list of
    # proposed boxes
    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()
    ss.setBaseImage(image)
    ss.switchToSelectiveSearchFast()
    rects = ss.process()
    proposedRoi = []
    proposedBoxes = []
    # rd_index = range(0, len(rects))
    # loop over the rectangles generated by selective search
    # for (x, y, w, h) in rects[random.sample(rd_index, round(0.1 * len(rects)))]:
    for (x, y, w, h) in rects:
        roi = image[y:y + h, x:x + w]
        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
        roi = cv2.resize(roi, config.INPUT_DIMS, interpolation=cv2.INTER_CUBIC)
        # further preprocess the ROI
        roi = img_to_array(roi)
        # preprocess_input may not be used as rois will be damaged somehow
        # roi = preprocess_input(roi)
        # update our proposedRoi and proposedBoxes lists
        proposedRoi.append(roi)
        proposedBoxes.append((x, y, x + w, y + h))
    # initialize counters used to count the number of positive
    positiveROIs = 0
    proposedRoi = np.array(proposedRoi, dtype="float32")
    label_proba = model.predict(proposedRoi)
    # apply NMS: 0-id of proposed roi; 1-id of class label
    # idx = np.where(label_proba >= config.MIN_PROBA)
    idx = np.argmax(label_proba, axis=1)
    # loop over the maximum number of region proposals and calculate ious
    ious = []
    for gtBox in gtBoxes:
        TP_count = 0
        for proposed_id, label_id_dt in enumerate(idx):
            # compute the intersection over union (iou) between each proposed boxes
            # and the ground-truth bounding box to decide whether it is TP or FP
            iou = compute_iou(gtBox[:-1], proposedBoxes[proposed_id])
            (gtStartX, gtStartY, gtEndX, gtEndY, label_id_gt) = gtBox
            if label_id_gt == label_id_dt:
                print(True)
                iou = 0
            # TP
            if iou >= MIN_IOU:
                TP_count += 1
                if TP_count == 1:
                    TP_FP[label_id_gt].append(1)
            # FP
            else:
                TP_FP[label_id_gt].append(0)
    # calculate # TP and # FP
    # for iou_info in ious:
    #     (iou, label_id_gt) = iou_info
    #     # TP
    #     if iou >= MIN_IOU:
    #         TP_count += 1
    #         if TP_count == 1:
    #             TP_FP[label_id_gt].append(1)
    #     # FP
    #     else:
    #         TP_FP[label_id_gt].append(0)

    # for gtBox in gtBoxes:
    #     TP_count = 0
    #     for proposed_id, label_id_dt in zip(idx[0], idx[1]):
    #         # compute the intersection over union (iou) between each proposed boxes
    #         # and the ground-truth bounding box to decide whether it is TP or FP
    #         iou = compute_iou(gtBox[:-1], proposedBoxes[proposed_id])
    #         (gtStartX, gtStartY, gtEndX, gtEndY, label_id_gt) = gtBox
    #         cat = cats[catIds.index(label_id_gt)]['name']
    #         # positive condition
    #         if iou > MIN_IOU:
    #             # true condition
    #             if cat == lb.classes_[label_id_dt]:
    #                 TP_count += 1
    #                 if TP_count == 1:
    #                     TP_FP[label_id_dt].append(1)
    #             # false condition
    #             else:
    #                 TP_FP[label_id_dt].append(0)
    # ious = []
    # for gtBox in gtBoxes:
    #     for proposed_id, label_id_dt in zip(idx[0], idx[1]):
    #         # compute the intersection over union (iou) between each proposed boxes
    #         # and the ground-truth bounding box to decide whether it is TP or FP
    #         iou = compute_iou(gtBox[:-1], proposedBoxes[proposed_id])
    #         (gtStartX, gtStartY, gtEndX, gtEndY, label_id_gt) = gtBox
    #         cat = cats[catIds.index(label_id_gt)]['name']
    #         if cat == lb.classes_[label_id_dt]:
    #             # positive condition
    #             if iou > MIN_IOU:
    #                 TP_FP[label_id_dt].append(1)
    #                 TP_FN[label_id_dt].append(1)
    #             # negative condition
    #             else:
    #                 TP_FP[label_id_dt].append(0)
    #         else:
    #             if iou <= MIN_IOU:
    #                 TP_FN[label_id_dt].append(0)

    # ious.append((iou, label_id_dt, cat))
    # ious.sort(key=lambda xx: xx[0], reverse=True)
    # for iou_info in ious[:50]:
    #     (iou, label_id_dt, cat) = iou_info
    #     # true condition
    #     if cat == lb.classes_[label_id_dt]:
    #         # positive condition
    #         if iou > MIN_IOU:
    #             TP_FP[label_id_dt].append(1)
    #             TP_FN[label_id_dt].append(1)
    #         # negative condition
    #         else:
    #             TP_FP[label_id_dt].append(0)
    #     else:
    #         if iou <= MIN_IOU:
    #             TP_FN[label_id_dt].append(0)

for j in range(len(TP_FP)):
    for k in range(len(TP_FP[j])):
        Precision[j].append(sum(TP_FP[j][0:k]) / (len(TP_FP[j][0:k]) + 1))
        Recall[j].append(sum(TP_FP[j][0:k]) / (TP_FN[j] + 1))
    AP[j] = compute_ap(Recall[j], Precision[j])
mAP = np.mean(AP, axis=0)
print("[INFO] saving AP...")
f = open('coco_ap_calc_1.pickle', "wb")
f.write(pickle.dumps(AP))
f.close()
class_no = [i+1 for i in range(80)]
# plot AP and print mAP
plt.style.use("ggplot")
plt.figure()
plt.title("Average Precision (with IoU threshold = " + str(MIN_IOU) + ")")
plt.plot(class_no, AP)
plt.xticks(rotation=300)
plt.xlabel("Class Labels")
plt.ylabel("AP")
plotPath = os.path.sep.join([config.PLOTS_PATH, "coco_ap_calc_1.png"])
plt.savefig(plotPath)
plt.close()
print("COCO's mAP is equal to " + str(mAP) + " (with IoU threshold = " + str(MIN_IOU) + ")")